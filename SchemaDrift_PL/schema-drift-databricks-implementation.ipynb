{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0daf9a0-8036-444c-8bd5-513d23b92ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**schema evolution** allows users to change schema to support changing data structure. This is common use case for data ingestion.\n",
    "\n",
    "**option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\")**\n",
    "- **failOnNewColumns** is STRICT schema implies will not accept any changes and stops the pipeline. change to either `addNewColumns` or `rescue` if you want to accept the evolution\n",
    "- **addNewColumns** will automatically addes new columns and continues\n",
    "- **rescue** doesnâ€™t evolve the schema. All unexpected fields fall into a special column called **_rescued_data** and stores data in JSON format\n",
    "\n",
    "**Note:**\n",
    "- `addNewColumns` mode is the **default** when a _**schema is not provided**_\n",
    "- **none** is the default when _**you provide a schema**_ (`addNewColumns` is not allowed when the schema of the streamis provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb1356e-bf31-4ec8-9ff1-29eb0343235e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets understand Expectations for **_rescued_data** IS NULL\n",
    "- No errors occur during ingestion related to schema mismatch or data parsing \n",
    "- all incoming data should align with the defined schema\n",
    "- Raise an alert / failure if we see any non conforming rows\n",
    "\n",
    "This ensures strict data quality enforcement to accept only valid schema rows and rows that are procerly parsed. Additionally, it also helps us to identify error detection in early stages of the pipeline\n",
    "\n",
    "**Note**: If _rescued_data IS NOT NULL implies that row is either does not adher to schema or some parsing error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee26a63-cadc-4d34-b510-87791be00fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Medallion architecture**\n",
    "- Bronze(dlt format) table -> Read from storage(aka raw data csv,tsv,json,xml,parquet,db source, etc) and create a bronze table which represents as-is data from source\n",
    "- Silver(dlt format) table -> Apply required exceptions/validations/schema datatype changes/business policies/etc\n",
    "\n",
    "**Files to be uploaded one after another for each run to show the demo**\n",
    "- customer_data_1.json: Base file(first file that we upload)\n",
    "- customer_data_2.json: Additional columns (age, gender,loyaltystatus) for existing customers values change plus new customer(s)\n",
    "- customer_data_3.json: No structure changes however existing customers value changes plus additional row(s)\n",
    "- customer_data_4.json: new column added (CreditScore) with existing customers value changes. No new row(s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3d4612-73e6-485d-92fc-93136cb91025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries management\n",
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "volume_path=\"/Volumes/workspace/sd_schema/datastore/customer_*.json\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af98c65e-2278-44b5-92e9-e6e15feba336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#bronze layer table: cust_bronze_sd\n",
    "pl.create_streaming_table(\"cust_bronze_sd_rescue\")\n",
    "\n",
    "# Ingest the raw data into the bronze table using append flow\n",
    "@pl.append_flow(\n",
    "  target = \"cust_bronze_sd_rescue\", #object name\n",
    "  name = \"cust_bronze_sd_rescue_ingest_flow\" #flow name\n",
    ")\n",
    "def cust_bronze_sd_rescue_ingest_flow():\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"json\")\n",
    "          .option(\"cloudFiles.inferColumnTypes\", \"true\") #auto scan schema \n",
    "          #.option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\") # schema customer_data_1.json is different than customer_data_2.json so it fails with  [UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH] excetion and stops processing\n",
    "          .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "          .load(f\"{volume_path}\")\n",
    "  )\n",
    "  return df.withColumn(\"ingestion_datetime\", current_timestamp())\\\n",
    "           .withColumn(\"source_filename\", col(\"_metadata.file_path\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4076be18-26f0-4280-bf78-07c9895dd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle DATATYPE changes\n",
    "# Logic to process the fields if data type changes. There are many ways it can be handled\n",
    "#    (*) Without Overwrite of data in silver layer\n",
    "#        Create additional table every time a colmn datatype changes and then create view on top of it as UNION to all new tables\n",
    "#            PROS: Technically we are not overwriting the data hence no reloading\n",
    "#            CONS: New table will created every time datatype changes \n",
    "#\n",
    "#    (*) Overwrite data in both Bronze and Silver layers\n",
    "#        I am not sure how this works for streams. I have not done much exploration in this method. Hope it works\n",
    "#        Here aswell we use _rescued_data column to check quality expectation for schema update\n",
    "#            PROS: No need to reload bronze layer table as _rescued_data has all desired changed and can be used to process\n",
    "#            CONS: Less code changes because _rescued_data doesnt need any additional logic to hendle. \n",
    "#                  However all raw data to be stored at begining. Reload of both tables\n",
    "#\n",
    "#    (*) Merge and overwrite data in silver layer (below function example does the same implementation)\n",
    "#            PROS: No need to reload bronze layer table as _rescued_data has all desired changed and can be used to process\n",
    "#            CONS: Table in silver need to be completely reloaded\n",
    "# NOTE: The above options technically doesnt handle column renames. We need to write additional logic to handle column renames\n",
    "#       I would say we could follow the views logic to load renamed column as new field and then in view drop old column and use new renamed column\n",
    "#       However, we need to merge the data in silver layer and hence we need to reload the silver layer table\n",
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema: StructType):\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_modified\", \n",
    "        from_json(col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "    )\n",
    "    \n",
    "    for field in target_schema.fields:\n",
    "        data_type = field.dataType\n",
    "        column_name = field.name\n",
    "        \n",
    "        if column_name in df.columns:\n",
    "            # Check if value is in rescued_data\n",
    "            key_condition = expr(f\"_rescued_data_modified IS NOT NULL AND map_contains_key(_rescued_data_modified, '{column_name}')\")\n",
    "            \n",
    "            # Use rescued value if present, otherwise use existing column\n",
    "            rescued_value = when(\n",
    "                key_condition, \n",
    "                col(\"_rescued_data_modified\").getItem(column_name).cast(data_type)\n",
    "            ).otherwise(col(column_name).cast(data_type))\n",
    "            \n",
    "            df = df.withColumn(column_name, rescued_value)\n",
    "    \n",
    "    df = df.drop('_rescued_data_modified')\n",
    "    df = df.withColumn('_rescued_data', lit(None).cast(StringType()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d83626-fd99-429c-8698-ac9c54420524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle adding NEW FIELDS \n",
    "def process__rescue_data_new_fields(df, expected_fields: list):\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_json_to_map\", \n",
    "        from_json(col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "    )\n",
    "    \n",
    "    for key in expected_fields:\n",
    "        if key != \"_file_path\":\n",
    "            if key not in df.columns:\n",
    "                df = df.withColumn(\n",
    "                    key,\n",
    "                    col(\"_rescued_data_json_to_map\").getItem(key)\n",
    "                )\n",
    "            else:\n",
    "                df = df.withColumn(\n",
    "                    key,\n",
    "                    when(\n",
    "                        col(\"_rescued_data_json_to_map\").getItem(key).isNotNull(),\n",
    "                        col(\"_rescued_data_json_to_map\").getItem(key)\n",
    "                    ).otherwise(col(key))\n",
    "                )\n",
    "    \n",
    "    df = df.drop('_rescued_data_json_to_map')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab95c81-916f-4934-93a5-e904cb31ca1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------------------------------\n",
    "# #plain implementation without processing _rescue_data field. Use this when you upload customer_data_1.json\n",
    "# # -----------------------------------------------------------------------------------------------------\n",
    "# pl.create_streaming_table(\n",
    "#   name = \"cust_silver_sd_rescue\",\n",
    "#   expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    "#   )\n",
    "# @pl.append_flow(\n",
    "#   target = \"cust_silver_sd_rescue\",\n",
    "#   name = \"cust_silver_sd_rescue_clean_flow\"\n",
    "# )\n",
    "# def cust_silver_sd_rescue_clean_flow():\n",
    "#   return (\n",
    "#       spark.readStream.table(\"cust_bronze_sd_rescue\")\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3de893-a152-4d8a-8830-fb8334a015af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "# uncomment this code before uploading customer_data_2.json. Then upload the file and run the pipeline\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# we know that when we process customer_data_2.json file there are new fields in schema to be added and at same time we are planing for datatype chage\n",
    "# for an already existing field that came with customer_data_1.json file. Since there is a datatype change for existing field so we need to perform\n",
    "# full refresh (Run pipeline with full table refresh)\n",
    "updated_datatypes = StructType([\n",
    "    StructField(\"SignupDate\", DateType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"CreditScore\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "expected_new_fields = [\n",
    "    # \"age\",             \n",
    "    # \"gender\",          \n",
    "    # \"loyaltyStatus\",   \n",
    "    \"CreditScore\"      \n",
    "]\n",
    "\n",
    "pl.create_streaming_table(\n",
    "  name = \"cust_silver_sd_rescue\",\n",
    "  expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"cust_silver_sd_rescue\",\n",
    "  name = \"cust_silver_sd_rescue_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_clean_flow():\n",
    "  df = (\n",
    "    spark.readStream.table(\"cust_bronze_sd_rescue\")\n",
    "  )\n",
    "  df = process__rescue_data_new_fields(df, expected_new_fields)\n",
    "  df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "  return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "schema-drift-databricks-implementation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
