{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0daf9a0-8036-444c-8bd5-513d23b92ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**schema evolution** allows users to change schema to support changing data structure. This is common use case for data ingestion.\n",
    "\n",
    "**option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\")**\n",
    "- **failOnNewColumns** is STRICT schema implies will not accept any changes and stops the pipeline. change to either `addNewColumns` or `rescue` if you want to accept the evolution\n",
    "- **addNewColumns** will automatically addes new columns and continues\n",
    "- **rescue** doesnâ€™t evolve the schema. All unexpected fields fall into a special column called **_rescued_data** and stores data in JSON format\n",
    "\n",
    "**Note:**\n",
    "- `addNewColumns` mode is the **default** when a _**schema is not provided**_\n",
    "- **none** is the default when _**you provide a schema**_ (`addNewColumns` is not allowed when the schema of the streamis provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb1356e-bf31-4ec8-9ff1-29eb0343235e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets understand Expectations for **_rescued_data** IS NULL\n",
    "- No errors occur during ingestion related to schema mismatch or data parsing \n",
    "- all incoming data should align with the defined schema\n",
    "- Raise an alert / failure if we see any non conforming rows\n",
    "\n",
    "This ensures strict data quality enforcement to accept only valid schema rows and rows that are procerly parsed. Additionally, it also helps us to identify error detection in early stages of the pipeline\n",
    "\n",
    "**Note**: If _rescued_data IS NOT NULL implies that row is either does not adher to schema or some parsing error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee26a63-cadc-4d34-b510-87791be00fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Medallion architecture**\n",
    "- Bronze(dlt format) table -> Read from storage(aka raw data csv,tsv,json,xml,parquet,db source, etc) and create a bronze table which represents as-is data from source\n",
    "- Silver(dlt format) table -> Apply required exceptions/validations/schema datatype changes/business policies/etc\n",
    "\n",
    "**Files to be uploaded one after another for each run to show the demo**\n",
    "- customer_data_1.json: Base file(first file that we upload)\n",
    "- customer_data_2.json: Additional columns (age, gender,loyaltystatus) for existing customers values change plus new customer(s)\n",
    "- customer_data_3.json: No structure changes however existing customers value changes plus additional row(s)\n",
    "- customer_data_4.json: new column added (CreditScore) with existing customers value changes. No new row(s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af3d4612-73e6-485d-92fc-93136cb91025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Libraries management\n",
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "volume_path=\"/Volumes/workspace/damg7370/datastore/schema_drift/demo_smm/customer_*.json\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af98c65e-2278-44b5-92e9-e6e15feba336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#bronze layer table: cust_bronze_sd\n",
    "pl.create_streaming_table(\"demo_cust_bronze_sd\")\n",
    "\n",
    "# Ingest the raw data into the bronze table using append flow\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_bronze_sd\", #object name\n",
    "  name = \"demo_cust_bronze_sd_ingest_flow\" #flow name\n",
    ")\n",
    "def demo_cust_bronze_sd_ingest_flow():\n",
    "  df = (\n",
    "      spark.readStream\n",
    "          .format(\"cloudFiles\")\n",
    "          .option(\"cloudFiles.format\", \"json\")\n",
    "          .option(\"cloudFiles.inferColumnTypes\", \"true\") #auto scan schema \n",
    "          #.option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\") # schema customer_data_1.json is different than customer_data_2.json so it fails with  [UNKNOWN_FIELD_EXCEPTION.NEW_FIELDS_IN_RECORD_WITH_FILE_PATH] excetion and stops processing\n",
    "          .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "          .load(f\"{volume_path}\")\n",
    "  )\n",
    "  return df.withColumn(\"ingestion_datetime\", current_timestamp())\\\n",
    "           .withColumn(\"source_filename\", col(\"_metadata.file_path\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4076be18-26f0-4280-bf78-07c9895dd296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle DATATYPE changes\n",
    "# Logic to process the fields if data type changes. There are many ways it can be handled\n",
    "#    (*) Without Overwrite of data in silver layer\n",
    "#        Create additional table every time a colmn datatype changes and then create view on top of it as UNION to all new tables\n",
    "#            PROS: Technically we are not overwriting the data hence no reloading\n",
    "#            CONS: New table will created every time datatype changes \n",
    "#\n",
    "#    (*) Overwrite data in both Bronze and Silver layers\n",
    "#        I am not sure how this works for streams. I have not done much exploration in this method. Hope it works\n",
    "#        Here aswell we use _rescued_data column to check qualit expectation for schema update\n",
    "#            PROS: No need to reload bronze layer table as _rescued_data has all desired changed and can be used to process\n",
    "#            CONS: Less code changes because _rescued_data doesnt need any additional logic to hendle. \n",
    "#                  However all raw data to be stored at begining. Reload of both tables\n",
    "#\n",
    "#    (*) Merge and overwrite data in silver layer (below function example does the same implementation)\n",
    "#            PROS: No need to reload bronze layer table as _rescued_data has all desired changed and can be used to process\n",
    "#            CONS: Table in silver need to be completely reloaded\n",
    "# NOTE: The above options technically doesnt handle column renames. We need to write additional logic to handle column renames\n",
    "#       I would say we could follow the views logic to load renamed column as new field and then in view drop old column and use new renamed column\n",
    "#       However, we need to merge the data in silver layer and hence we need to reload the silver layer table\n",
    "\n",
    "def process__rescue_data_datatype_change(df, target_schema: StructType):\n",
    "    #Parse the _rescued_data json to a MAP (Key,Value) type and store in _rescued_data_modified column\n",
    "    df = df.withColumn(\"_rescued_data_modified\", from_json(col(\"_rescued_data\"), MapType(StringType(), StringType())))\n",
    "    \n",
    "    for field in target_schema.fields:\n",
    "        data_type = field.dataType\n",
    "        column_name = field.name\n",
    "\n",
    "        # Check if \"_rescue_data\" is not null and if the key exists\n",
    "        # pyspark.sql.functions.map_contains_key function in PySpark is used to check if a specified key exists within a MapType column in a DataFrame. returns T/F\n",
    "        key_condition = expr(f\"_rescued_data_modified IS NOT NULL AND map_contains_key(_rescued_data_modified, '{column_name}')\")\n",
    "        \n",
    "        # Extract the rescued value for this column, if it exists, and cast it to the target data type\n",
    "        rescued_value = when(key_condition, col(\"_rescued_data_modified\").getItem(column_name).cast(data_type)).otherwise(col(column_name).cast(data_type))\n",
    "        \n",
    "        # Update the DataFrame with the merged column\n",
    "        df = df.withColumn(column_name, rescued_value)\n",
    "        df = df.withColumn(column_name, col(column_name).cast(data_type))\n",
    "        \n",
    "    df = df.drop('_rescued_data_modified')\n",
    "\n",
    "    # Setting the _rescued_data to null after processing since we use the column to check qualit expectation for schema update\n",
    "    df = df.withColumn('_rescued_data', lit(None).cast(StringType()))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d83626-fd99-429c-8698-ac9c54420524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle adding NEW FIELDS \n",
    "def process__rescue_data_new_fields(df):\n",
    "\n",
    "    #Add all fields from _rescued_data to key map\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_json_to_map\", \n",
    "        from_json(\n",
    "            col(\"_rescued_data\"), \n",
    "            MapType(StringType(), StringType())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Extract all keys from _rescued_data_map_keys\n",
    "    df = df.withColumn(\"_rescued_data_map_keys\", map_keys(col(\"_rescued_data_json_to_map\")))\n",
    "\n",
    "    # Get all keys in all rows as a new DataFrame\n",
    "    df_keys = df.select(\n",
    "        explode(\n",
    "            map_keys(col(\"_rescued_data_json_to_map\"))\n",
    "        ).alias(\"rescued_key\")\n",
    "    ).distinct()\n",
    "\n",
    "    # Collect keys as a list (only if df is not streaming)\n",
    "    # If streaming, you must provide the list of possible keys another way\n",
    "    new_keys = [row[\"rescued_key\"] for row in df_keys.collect()] if not df.isStreaming else []\n",
    "\n",
    "    # Add new columns for each key\n",
    "    for key in new_keys:\n",
    "        if key != \"_file_path\":\n",
    "            df = df.withColumn(\n",
    "                key,\n",
    "                col(\"_rescued_data_json_to_map\").getItem(key).cast(StringType())\n",
    "            )\n",
    "\n",
    "    #***Ehnancement can be done by adding additional logic \n",
    "    #***  to exclude columns that are already in dataframe(Substract those columns)\n",
    "    #***  to infer datatype for new columns and use infered datatype instead of static stringtype\n",
    "    #***  additionally check if each column exists and dataframe has rows on each transformation and raise exception before using it\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ab95c81-916f-4934-93a5-e904cb31ca1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # -----------------------------------------------------------------------------------------------------\n",
    "# #plain implementation without processing _rescue_data field. Use this when you upload customer_data_1.json\n",
    "# # -----------------------------------------------------------------------------------------------------\n",
    "# pl.create_streaming_table(\n",
    "#   name = \"demo_cust_silver_sd\",\n",
    "#   expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    "#   )\n",
    "# @pl.append_flow(\n",
    "#   target = \"demo_cust_silver_sd\",\n",
    "#   name = \"demo_cust_silver_sd_clean_flow\"\n",
    "# )\n",
    "# def demo_cust_silver_sd_clean_flow():\n",
    "#   return (\n",
    "#       spark.readStream.table(\"demo_cust_bronze_sd\")\n",
    "#   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3de893-a152-4d8a-8830-fb8334a015af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------------\n",
    "# uncomment this code before uploading customer_data_2.json. Then upload the file and run the pipeline\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "# we know that when we process customer_data_2.json file there are new fields in schema to be added and at same time we are planing for datatype chage\n",
    "# for an already existing field that came with customer_data_1.json file. Since there is a datatype change for existing field so we need to perform\n",
    "# full refresh (Run pipeline with full table refresh)\n",
    "updated_datatypes = StructType([\n",
    "  # define the column signuoDate as DATE type and also make it nullable (Make 3rd argument False if you want to make it non nullable)\n",
    "  StructField(\"signupDate\", DateType(), True) \n",
    "])\n",
    "\n",
    "pl.create_streaming_table(\n",
    "  name = \"demo_cust_silver_sd\",\n",
    "  expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_silver_sd\",\n",
    "  name = \"demo_cust_silver_sd_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_clean_flow():\n",
    "  df = (\n",
    "    spark.readStream.table(\"demo_cust_bronze_sd\")\n",
    "  )\n",
    "  df = process__rescue_data_new_fields(df)\n",
    "  df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "  return df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "schema-drift-databricks-implementation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
